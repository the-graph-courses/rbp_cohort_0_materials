---
title: "Honors Assignment: Extracting and Exporting Data Subsets"
output:
  prettydoc::html_pretty:
    theme: architect
    toc: true
    number_sections: true
author: "Author's name here"
date: "2022-11-01"
editor_options: 
  chunk_output_type: console
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Intro

Welcome!

Today, for this Honor's task, you will be carrying out a new task as a data analyst: preparing data subsets for someone else to use. It is a hands-on approach to using the `select()` and `filter()` verbs.

The assignment should be submitted individually, but you are encouraged to brainstorm with partners.

The final due date for the assignment is Tuesday, November 28th at 23:59 PM UTC+2.

# Obtain the course repo

1.  First download the course repo [here](https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/the-graph-courses/rbp_cohort_0_materials/tree/main/week_04){target="_blank"}.

    (You should ideally work on your local computer, but if you would rather work on RStudio Cloud, you can upload the zip file to RStudio Cloud through the Files pane. Consult one of the instructors for guidance on this.)

2.  Unzip/Extract the downloaded folder.

    On macOS, you can simply double-click on a file to unzip it.

    If you are on Windows and are not sure how to "unzip" a file, see [this image](https://imgur.com/a/6e5pT7k){target="_blank"}. You need to right-click on the file and then select "extract all".

3.  Once the unzipping is done, click on the RStudio Project file in the unzipped folder to open the project in RStudio.

4.  When your project is open in RStudio, navigate to the Files tab and open the "rmd" folder. The instructions for your exercise are outlined there (these are the same instructions you see here).

5.  Open the "data" folder, and observe its components. You will work with the "rabies_dataset.csv" file. (You can also view the "00_info_about_the_dataset" file to learn more about this data.)

6.  In the same folder, open the metadata (data dictionary) file for this dataset, "rabies_metadata.pdf". This file is necessary because the data for today is numerically encoded (all variables are stored as numbers which correspond to categories). Without the variable dictionary, you cannot know what the numeric categories correspond to!

# Load and clean the data

Now that you understand the structure of the repo, you should load in and clean your dataset.

In the code section below, **load in the packages** (hint: tidyverse) you'll need. Pro tip: Use `p_load()` to load in your packages, since it both loads and installs packages as needed.

```{r}
"WRITE_YOUR_CODE_HERE"
```

Now, **read in the dataset**. The data frame you import should have 1466 rows and 23 columns.Try to write a *relative* path that will work on anyone's computer.

```{r}
"WRITE_YOUR_CODE_HERE"
```

Next, perform the following two cleaning tasks on the imported dataset, then store the cleaned dataset in a new object.

-   **Bring the respondent ID to the front**. The respondent ID is the 23rd column of your dataset: we would like it to be more visible so that someone who opens up the CSV knows immediately that each row corresponds to a respondent. Move it to the first position in the data frame.

-   **Remove the `Education` variable, which has not been properly encoded.** (To notice the encoding issues, look at both the data frame and the metadata file). We can consider this variable unusable, and hence remove it.

```{r}
"WRITE_YOUR_CODE_HERE"
```

# Create and export data subsets

In each "Data subset" section below, you should

-   decide whether to use the `filter()` or `select()` function, then apply that function to create the required extract of the dataset;

-   then export the data subset into an appropriately-named CSV file in the "data_exports" folder.

-   NOTE: For all data subsets, always keep the respondent ID variable! Without it, you cannot link back your data to the original dataset and you lose crucial information.

## **Data Subset 1:** Extract demographic information

Create and export a data subset of the respondents' demographic information---their age, gender and geographic background.

```{r}
"WRITE_YOUR_CODE_HERE"
```

(Don't forget that you were asked to include the respondent ID variable in all subsets!)

## **Data Subset 2:** Extract all male adults from the dataset

Create and export a data subset with only males aged over 18.

```{r}
"WRITE_YOUR_CODE_HERE"
```

(Hint: this is a row-filtering question, so there is no need to select or drop columns.)

## **Data Subset 3:** Extract at-risk individuals

Create and export a subset with "at-risk" individuals. These are people who a) have a pet at home, b) have no access to health facilities, and c) consider that the rabies vaccine is not affordable for them.

```{r}
"WRITE_YOUR_CODE_HERE"
```

## **Data Subset 4:** Extract the knowledge-evaluation survey question variables

Reading the metadata, you will see that some variables correspond to "knowledge-evaluation" questions about rabies. Create and export a subset that includes these variables.

```{r}
"WRITE_YOUR_CODE_HERE"
```

## **Data Subset 5:** Extract respondents with "ideal" knowledge, attitudes and practices (KAPs) towards rabies

People with *ideal* KAPs are defined as people who answered that they:

-   vaccinate their pets,
-   know the clinical signs for rabies, and
-   visit a doctor after being bitten by an animal

Create and export a data subset that includes just these individuals.

```{r}
"WRITE_YOUR_CODE_HERE"
```

# Submission: share rpubs link, and upload zipped folder

Once you have finished the tasks above, you should

-   **Knit your document** with global code echo turned on. (You are turning on code echo so that your written code is visible in the output document).

-   Once knitted, **publish the document** to [rpubs](https://rpubs.com/){target="_blank"}, then **share the rpubs link** as a comment on the assignment page. (Not sure how to publish to rpubs? See [this video](https://www.youtube.com/watch?v=GJ36zamYVLg){target="_blank"}.)

-   Next, **create a zipped file** of this "week_04" folder, containing all your work, and **upload it** on the assignment page.
